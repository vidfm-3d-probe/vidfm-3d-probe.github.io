<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="We propose a model-agnostic probing framework to quantify how much 3D structure and ego-motion video foundation models encode, using shallow feedforward readouts for point maps, depth, and camera poses.">
  <meta name="keywords" content="Video Foundation Models, VidFM, 3D awareness, probing, video diffusion, 3D reconstruction, depth, camera pose, point map">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>How Much 3D Do Video Foundation Models Encode?</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="fav/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="fav/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="fav/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="fav/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="fav/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="fav/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="fav/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="fav/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="fav/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="fav/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="fav/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="fav/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="fav/favicon-16x16.png">
  <link rel="manifest" href="fav/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="fav/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script>
    function toggleModels(modelNames) {
      const iframe = document.querySelector('iframe[src="radar_vidfm.html"]');
      if (iframe && iframe.contentWindow) {
        iframe.contentWindow.postMessage({
          type: 'toggleModels',
          models: modelNames
        }, '*');
        
        // Scroll to the chart smoothly
        iframe.scrollIntoView({ behavior: 'smooth', block: 'center' });
      }
    }
  </script>
</head>

<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">How Much 3D Do Video Foundation Models Encode?</h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zixuanh.com">Zixuan Huang</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://ryanxli.github.io">Xiang Li</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://lvzhaoyang.github.io/">Zhaoyang Lv</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://rehg.org">James M. Rehg</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1&nbsp;</sup>University of Illinois at Urbana-Champaign</span>
            <span>&nbsp;&nbsp;&nbsp;&nbsp;</span>
            <span class="author-block"><sup>2&nbsp;</sup>Impossible, Inc.</span>
          </div>

          <div class="is-size-6 publication-authors" style="margin-top: 0.5em;">
            <span class="author-block"><sup>*</sup>Equal contribution</span>
          </div>

          <div class="column has-text-centered" style="margin-top: 0.2rem;">
            <div class="publication-links">

              <!-- Paper PDF -->
              <span class="link-block">
                <a href="./static/assets/vidfm_3d_probe.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>Paper</span>
                </a>
              </span>

              <span>&nbsp;&nbsp;&nbsp;&nbsp;</span>

              <!-- arXiv
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>

              <span>&nbsp;</span>

              <!-- Code -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code (Coming Soon)</span>
                </a>
              </span>


              <!-- Project
              <span class="link-block">
                <a href="https://vidfm-3d-probe.github.io/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-globe"></i></span>
                  <span>Project Page</span>
                </a>
              </span> -->

            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser / TL;DR -->
<section class="section" style="padding-top: 0rem; margin-top: -2.5rem;">
  <div class="container is-max-desktop">
    <div class="hero-body" style="padding-top: 0.5rem; padding-bottom: 1rem;">

      <img src="./static/assets/teaser.png" alt="Teaser figure (3D probing of VidFMs)" style="width: 100%; border-radius: 12px;">
      <br>

      <div class="content has-text-left">
        <p>
          <strong>TL;DR:</strong>
          After training on large 2D videos, will video foundation models (VidFMs) naturally encode 3D structure and ego-motion? Our study reveals that state-of-the-art video generators develop strong, generalizable 3D understanding even compared to 3D experts, despite being trained only on 2D video data.
        </p>
      </div>

    </div>
  </div>
</section>


<!-- Abstract -->
<section class="section has-background-light" style="padding-top: 0rem; margin-top: 0rem;">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <h2 class="title is-3">Abstract</h2>
      <div class="content has-text-left">
        <p>
          Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. 
        </p><p>
          Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.
        </p>
      </div>
    </div>
  </div>
</section>


<!-- Approach -->
<section class="section" style="padding-top: 0rem; margin-top: 0rem;">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">Feedforward Probe of 3D Awareness</h2>

      <img src="./static/assets/probe.png" alt="Probe overview (feature extraction + shallow transformer probe)" style="width: 100%; border-radius: 12px;">
      <br><br>

      <div class="content has-text-left">
        <p>
          We design a <em>model-agnostic probing framework</em> that reads out
          <strong>3D point maps</strong>, <strong>depth maps</strong>, and <strong>camera poses</strong>
          from frozen VidFM features using a shallow feedforward probe. The probing results quantify how much 3D is encoded in VidFMs' feature space, which we define as 3D awareness.
        </p>


        <p>
          Specifically, we extract video features using various video foundation models and keep the features frozen. We sample four frames from the original video clip and fetch the corresponding feature maps from the video features. We train the probe by taking these spatial features as input, and task the probe to estimate point maps, depth maps and camera poses. We measure the estimation errors as the main indicators of 3D awareness.
        </p>

        <p>
          <strong>Probe architecture:</strong> a shallow transformer with 4 alternating attentions + 3 readout heads (points, depth, pose).
        </p>

        <p>
          <strong>Benchmarked models:</strong> video diffusion models (e.g., WAN2.1, Open-Sora2.0), self-supervised video models (V-JEPA), image models (DINOv2) and 3D experts (Fast3R).
        </p>

        <p>
          <strong>Datasets:</strong> CO3Dv2 (objects) and DL3DV (large scenes).
        </p>

        <p>
          <strong>Metrics:</strong> point-map error, depth error, and pose AUC at multiple thresholds.
        </p>
      </div>
    </div>
  </div>
</section>


<!-- Benchmark -->
<section class="section has-background-light" style="padding-top: 0rem; margin-top: 0rem;">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">Benchmark Results</h2>

    <div class="content has-text-left">
      <p>
        We compare video diffusion models (<strong>CogVideoX</strong>, <strong>Aether</strong>, <strong>Open-Sora2.0</strong>, <strong>WAN2.1-14B</strong>),
        a self-supervised video encoder (<strong>V-JEPA</strong>), and two control baselines:
        <strong>DINOv2</strong> (per-frame image features) and <strong>Fast3R</strong> (native 3D expert).
        Strong video generators (especially <strong>WAN2.1-14B</strong> and <strong>Open-Sora2.0</strong>) show high 3D awareness.
      </p>
    </div>

    <div class="column">
      <iframe src="radar_vidfm.html" 
              style="width: 100%; height: 700px; border: none; overflow: hidden;"
              scrolling="no"
              title="Unified Evaluation of Single-Image 3D Generation Models">
      </iframe>
      <p class="has-text-centered is-size-7" style="margin-top: 0.5rem; color: #666;">
        <em>Interactive chart: Click legend items to show/hide models ‚Ä¢ Hover over lines for detailed metrics ‚Ä¢ Use toolbar buttons to select/deselect all or save as image</em>
      </p>
    </div>

<!-- 
    <div class="table-container">
      <table class="table is-striped is-hoverable is-fullwidth">
        <thead>
          <tr>
            <th rowspan="2">Probed Feature</th>
            <th colspan="4" class="has-text-centered">CO3Dv2</th>
            <th colspan="4" class="has-text-centered">DL3DV</th>
          </tr>
          <tr>
            <th>Point Err (‚Üì)</th>
            <th>Depth Err (‚Üì)</th>
            <th>AUC@5 (‚Üë)</th>
            <th>AUC@30 (‚Üë)</th>
            <th>Point Err (‚Üì)</th>
            <th>Depth Err (‚Üì)</th>
            <th>AUC@5 (‚Üë)</th>
            <th>AUC@30 (‚Üë)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>DINOv2</td>
            <td>0.559</td><td>0.209</td><td>0.051</td><td>0.508</td>
            <td>2.814</td><td>0.534</td><td>0.013</td><td>0.245</td>
          </tr>
          <tr>
            <td>V-JEPA</td>
            <td>0.439</td><td>0.214</td><td>0.076</td><td>0.619</td>
            <td>1.576</td><td>0.613</td><td>0.076</td><td>0.558</td>
          </tr>
          <tr>
            <td>CogVideoX</td>
            <td>0.485</td><td>0.231</td><td>0.051</td><td>0.569</td>
            <td>1.748</td><td>0.608</td><td>0.061</td><td>0.486</td>
          </tr>
          <tr>
            <td>Aether</td>
            <td>0.501</td><td>0.249</td><td>0.054</td><td>0.571</td>
            <td>1.566</td><td>0.574</td><td>0.067</td><td>0.527</td>
          </tr>
          <tr>
            <td>Open-Sora2.0</td>
            <td>0.391</td><td>0.196</td><td>0.096</td><td>0.643</td>
            <td>1.306</td><td>0.445</td><td>0.115</td><td>0.607</td>
          </tr>
          <tr>
            <td><strong>WAN2.1-14B</strong></td>
            <td><strong>0.284</strong></td><td><strong>0.151</strong></td><td><strong>0.200</strong></td><td><strong>0.736</strong></td>
            <td><strong>1.051</strong></td><td><strong>0.323</strong></td><td><strong>0.136</strong></td><td><strong>0.660</strong></td>
          </tr>
          <tr>
            <td>Fast3R</td>
            <td>0.262</td><td>0.145</td><td>0.272</td><td>0.769</td>
            <td>1.379</td><td>0.514</td><td>0.134</td><td>0.637</td>
          </tr>
        </tbody>
      </table>
    </div> -->

    <div class="content has-text-left">
      <h3 class="title is-5" style="margin-top: 2rem;">Main Findings</h3>
      <ul style="list-style: none; padding-left: 0;">
        <li style="margin-bottom: 1.5rem;">
          <strong>Frontier video diffusion/generation models encode surprisingly strong global 3D structure and ego-motion‚Äîoften rivaling or surpassing 3D specialists.</strong> Concretely, WAN2.1-14B is close to Fast3R on CO3Dv2, and on DL3DV it surpasses Fast3R across point/depth/pose metrics; Open-Sora2.0 is also consistently strong, suggesting top video generators yield broadly transferable 3D-aware features across domains.
          <br><button class="button is-small is-link is-light" style="margin-top: 0.5rem;" onclick="toggleModels(['WAN2.1-14B', 'Open-Sora2.0', 'Fast3R'])">üìä Compare: WAN2.1-14B, Open-Sora2.0, Fast3R</button>
        </li>
        <li style="margin-bottom: 1.5rem;">
          <strong>Effect of temporal reasoning: Temporal information exchange is critical for global 3D understanding such as 3D points and camera poses.</strong> Per-frame DINOv2 remains competitive on depth, but is much worse on global 3D metrics than video models including self-supervised ones like V-JEPA.
          <br><button class="button is-small is-link is-light" style="margin-top: 0.5rem;" onclick="toggleModels(['DINOv2', 'V-JEPA', 'WAN2.1-14B'])">üìä Compare: DINOv2, V-JEPA, WAN2.1-14B</button>
        </li>
        <li style="margin-bottom: 1.5rem;">
          <strong>Effect of 3D fine-tuning: Fine-tuning a video generator with 3D-aware objectives can improve 3D awareness in-domain, but may hurt generalization across domains.</strong> Aether (3D-aware fine-tuned from CogVideoX) improves over CogVideoX on DL3DV, yet is slightly worse on CO3Dv2. This likely relates to that Aether is finetuned mainly with large synthetic scenes (games/simulators).
          <br><button class="button is-small is-link is-light" style="margin-top: 0.5rem;" onclick="toggleModels(['Aether', 'CogVideoX'])">üìä Compare: Aether vs CogVideoX</button>
        </li>
        <li style="margin-bottom: 1.5rem;">
          <strong>Effect of scaling: Scaling does not monotonically increase 3D awareness.</strong> WAN improves a lot when scaling from 1.3B to 14B, while CogVideoX slightly worsens from 2B to 5B. This shows that parameter count alone isn't a guarantee of 3D awareness, and we hypothesize that training data plays a key role here (e.g., WAN's scaling up includes additional high-quality/high-resolution data).
          <br><button class="button is-small is-link is-light" style="margin-top: 0.5rem;" onclick="toggleModels(['WAN2.1-14B', 'WAN2.1-1.3B'])">üìä Compare: WAN2.1 Scaling</button>
        </li>
      </ul>
    </div>

    </div>
  </div>
</section>

<!-- Qualitative Results (Fig. 3 & 4) -->
<section class="section" id="qualitative" style="padding-top: 0rem; margin-top: 0rem;">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">Qualitative Results</h2>
      <p class="subtitle is-6 has-text-left" style="max-width: 900px;">
        We visualize reconstructed 3D point clouds from frozen VidFM features with our shallow probe.
      </p>
    </div>

    <!-- Fig 3 -->
    <div class="content">

      <div id="qualitative-carousel" class="carousel qualitative-carousel" style="max-width: 1100px; margin: 0 auto;">
        <div class="item item-qual1">
          <figure class="image">
            <img src="./static/assets/qual1.png"
                 alt="Figure 3: CO3Dv2 qualitative results (input frames + unprojected 3D points across models)"
                 style="width: 100%; border-radius: 12px;">
          </figure>
        </div>
        <div class="item item-qual2">
          <figure class="image">
            <img src="./static/assets/qual2.png"
                 alt="Figure 3: CO3Dv2 qualitative results (input frames + unprojected 3D points across models)"
                 style="width: 100%; border-radius: 12px;">
          </figure>
        </div>
        <div class="item item-qual3">
          <figure class="image">
            <img src="./static/assets/qual3.png"
                 alt="Figure 3: CO3Dv2 qualitative results (input frames + unprojected 3D points across models)"
                 style="width: 100%; border-radius: 12px;">
          </figure>
        </div>
        <div class="item item-qual4">
          <figure class="image">
            <img src="./static/assets/qual4.png"
                 alt="Figure 3: CO3Dv2 qualitative results (input frames + unprojected 3D points across models)"
                 style="width: 100%; border-radius: 12px;">
          </figure>
        </div>
      </div>

      <div class="content has-text-left" style="max-width: 1100px; margin: 1rem auto 0;">
        <p>
          The per-frame image baseline (<strong>DINOv2</strong>) can fail catastrophically on difficult scenes,
          while top video generators often retain coherent geometry. Among them, <strong>WAN2.1-14B</strong> typically produces the
          sharpest and most accurate point clouds overall. 
        </p>
      </div>
    </div>

  </div>
</section>


<!-- Interactive 3D Visualizations -->
<section class="section has-background-light" style="padding-top: 0rem; margin-top: 0rem;">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">Interactive 3D Visualizations</h2>
      
      <div class="content has-text-left">
        <p>
          Explore the reconstructed 3D point clouds interactively. Select a scene and a method below to view the 3D reconstruction. Rotate, zoom, and pan to examine the geometry and camera poses from different viewpoints.
        </p>
      </div>

      <!-- Scene Selection -->
      <div style="margin-top: 2rem;">
        <h3 class="title is-5" style="margin-bottom: 1rem;">Select Scene:</h3>
        <div class="columns is-mobile" style="flex-wrap: nowrap; overflow-x: auto;">
          <div class="column is-narrow" style="padding: 0.5rem;">
            <div class="scene-thumb" data-scene="1" style="cursor: pointer; border: 3px solid #3273dc; border-radius: 8px; overflow: hidden; transition: all 0.2s;">
              <img src="viser-scenes/images/scene1.png" alt="Scene 1" style="width: 150px; height: 150px; object-fit: cover; display: block;">
              <p style="text-align: center; font-size: 0.8rem; padding: 0.3rem; background: #3273dc; color: white; margin: 0;"><strong>Scene 1</strong></p>
            </div>
          </div>
          <div class="column is-narrow" style="padding: 0.5rem;">
            <div class="scene-thumb" data-scene="2" style="cursor: pointer; border: 3px solid transparent; border-radius: 8px; overflow: hidden; transition: all 0.2s;">
              <img src="viser-scenes/images/scene2.png" alt="Scene 2" style="width: 150px; height: 150px; object-fit: cover; display: block;">
              <p style="text-align: center; font-size: 0.8rem; padding: 0.3rem; background: #ddd; color: #333; margin: 0;">Scene 2</p>
            </div>
          </div>
          <div class="column is-narrow" style="padding: 0.5rem;">
            <div class="scene-thumb" data-scene="3" style="cursor: pointer; border: 3px solid transparent; border-radius: 8px; overflow: hidden; transition: all 0.2s;">
              <img src="viser-scenes/images/scene3.png" alt="Scene 3" style="width: 150px; height: 150px; object-fit: cover; display: block;">
              <p style="text-align: center; font-size: 0.8rem; padding: 0.3rem; background: #ddd; color: #333; margin: 0;">Scene 3</p>
            </div>
          </div>
          <div class="column is-narrow" style="padding: 0.5rem;">
            <div class="scene-thumb" data-scene="4" style="cursor: pointer; border: 3px solid transparent; border-radius: 8px; overflow: hidden; transition: all 0.2s;">
              <img src="viser-scenes/images/scene4.png" alt="Scene 4" style="width: 150px; height: 150px; object-fit: cover; display: block;">
              <p style="text-align: center; font-size: 0.8rem; padding: 0.3rem; background: #ddd; color: #333; margin: 0;">Scene 4</p>
            </div>
          </div>
          <div class="column is-narrow" style="padding: 0.5rem;">
            <div class="scene-thumb" data-scene="5" style="cursor: pointer; border: 3px solid transparent; border-radius: 8px; overflow: hidden; transition: all 0.2s;">
              <img src="viser-scenes/images/scene5.png" alt="Scene 5" style="width: 150px; height: 150px; object-fit: cover; display: block;">
              <p style="text-align: center; font-size: 0.8rem; padding: 0.3rem; background: #ddd; color: #333; margin: 0;">Scene 5</p>
            </div>
          </div>
        </div>
      </div>

      <!-- Method Selection -->
      <div style="margin-top: 2rem;">
        <h3 class="title is-5" style="margin-bottom: 1rem;">Select Method:</h3>
        <div style="display: flex; flex-wrap: wrap; gap: 0.5rem; justify-content: center;">
          <button class="method-btn" data-method="dino" data-name="DINOv2" style="padding: 0.6rem 1.2rem; border: 2px solid #8da0cb; background: #8da0cb; color: white; border-radius: 6px; cursor: pointer; font-weight: bold; transition: all 0.2s; font-size: 0.9rem;">
            DINOv2
          </button>
          <button class="method-btn" data-method="vjepa" data-name="V-JEPA" style="padding: 0.6rem 1.2rem; border: 2px solid #ffd92f; background: white; color: #333; border-radius: 6px; cursor: pointer; font-weight: bold; transition: all 0.2s; font-size: 0.9rem;">
            V-JEPA
          </button>
          <button class="method-btn" data-method="cogvideox" data-name="CogVideoX" style="padding: 0.6rem 1.2rem; border: 2px solid #fc8d62; background: white; color: #333; border-radius: 6px; cursor: pointer; font-weight: bold; transition: all 0.2s; font-size: 0.9rem;">
            CogVideoX
          </button>
          <button class="method-btn" data-method="aether" data-name="Aether" style="padding: 0.6rem 1.2rem; border: 2px solid #66c2a5; background: white; color: #333; border-radius: 6px; cursor: pointer; font-weight: bold; transition: all 0.2s; font-size: 0.9rem;">
            Aether
          </button>
          <button class="method-btn" data-method="opensora" data-name="Open-Sora2.0" style="padding: 0.6rem 1.2rem; border: 2px solid #a6d854; background: white; color: #333; border-radius: 6px; cursor: pointer; font-weight: bold; transition: all 0.2s; font-size: 0.9rem;">
            Open-Sora2.0
          </button>
          <button class="method-btn" data-method="wan" data-name="WAN2.1-1.3B" style="padding: 0.6rem 1.2rem; border: 2px solid #b3b3b3; background: white; color: #333; border-radius: 6px; cursor: pointer; font-weight: bold; transition: all 0.2s; font-size: 0.9rem;">
            WAN2.1-1.3B
          </button>
          <button class="method-btn" data-method="wan_14b" data-name="WAN2.1-14B" style="padding: 0.6rem 1.2rem; border: 2px solid #e5c494; background: white; color: #333; border-radius: 6px; cursor: pointer; font-weight: bold; transition: all 0.2s; font-size: 0.9rem;">
            WAN2.1-14B
          </button>
          <button class="method-btn" data-method="f3r" data-name="Fast3R" style="padding: 0.6rem 1.2rem; border: 2px solid #e78ac3; background: white; color: #333; border-radius: 6px; cursor: pointer; font-weight: bold; transition: all 0.2s; font-size: 0.9rem;">
            Fast3R
          </button>
          <button class="method-btn" data-method="gt" data-name="Groundtruth" style="padding: 0.6rem 1.2rem; border: 2px solid #333; background: white; color: #333; border-radius: 6px; cursor: pointer; font-weight: bold; transition: all 0.2s; font-size: 0.9rem;">
            Groundtruth
          </button>
        </div>
      </div>

      <!-- Visualization Container -->
      <div style="margin-top: 2rem;">
        <div style="background: white; padding: 1rem; border-radius: 8px; border: 1px solid #ddd;">
          <h4 class="subtitle is-5 has-text-centered" style="margin-bottom: 1rem;">
            <span id="current-method-name">DINOv2</span> - <span id="current-scene-name">Scene 1</span>
          </h4>
          <iframe id="viser-frame" 
                  src="viser-client/?playbackPath=../viser-scenes/exported/dino_co3d_scene1.viser&initialCameraPosition=0.020696,-0.188475,-1.277273&initialCameraQuaternion=0.99726511,-0.0732301,-0.00755666,0.0065228" 
                  style="width: 100%; height: 600px; border: none; border-radius: 8px;"
                  title="3D Reconstruction Viewer">
          </iframe>
        </div>
        <p class="has-text-centered is-size-7" style="margin-top: 0.5rem; color: #666;">
          <em>üí° Tip: Click and drag to rotate ‚Ä¢ Scroll to zoom ‚Ä¢ Right-click drag to pan ‚Ä¢ Toggle camera frustums with the checkbox on the right</em>
        </p>
      </div>

    </div>
  </div>
</section>

<script>
  // Interactive 3D Visualization Selector
  (function() {
    // Method colors from radar chart
    const methodColors = {
      "dino": '#8da0cb',
      "vjepa": '#ffd92f',
      "cogvideox": '#fc8d62',
      "aether": '#66c2a5',
      "opensora": '#a6d854',
      "wan": '#b3b3b3',
      "wan_14b": '#e5c494',
      "f3r": '#e78ac3',
      "gt": '#333'
    };

    // Scene to file mapping
    // Scenes 1-2 are CO3D scene1-2
    // Scenes 3-5 are DL3DV scene1-3
    const sceneMapping = {
      "1": { dataset: "co3d", sceneNum: "1" },
      "2": { dataset: "co3d", sceneNum: "2" },
      "3": { dataset: "dl3dv", sceneNum: "1" },
      "4": { dataset: "dl3dv", sceneNum: "2" },
      "5": { dataset: "dl3dv", sceneNum: "3" }
    };

    // Camera parameters for each scene (position and quaternion)
    const sceneCameraParams = {
      "1": {
        position: [0.020696, -0.188475, -1.277273],
        quaternion: [0.99726511, -0.0732301, -0.00755666, 0.0065228]
      },
      "2": {
        position: [-0.039, -0.341, -1.241],
        quaternion: [0.13428561, 0.98930734, -0.05638949, -0.00765414]
      },
      "3": {
        position: [-0.022, -0.164, -1.175],
        quaternion: [0.06974001, 0.99533780, -0.06646279, -0.00465683]
      },
      "4": {
        position: [-0.066, -0.660, -1.507],
        quaternion: [0.20557497, 0.97737277, -0.04874707, -0.01025318]
      },
      "5": {
        position: [-0.012, -0.167, -1.388],
        quaternion: [0.05994898, 0.99755715, -0.03579423, -0.00215108]
      }
    };

    let currentScene = "1";
    let currentMethod = "dino";

    // Update visualization
    function updateVisualization() {
      const mapping = sceneMapping[currentScene];
      const camera = sceneCameraParams[currentScene];
      
      // Build camera parameter string
      const cameraParams = 
        `initialCameraPosition=${camera.position.join(',')}&` +
        `initialCameraQuaternion=${camera.quaternion.join(',')}`;
      
      // Handle groundtruth vs method filenames
      let viserPath;
      if (currentMethod === "gt") {
        viserPath = `viser-client/?playbackPath=../viser-scenes/exported/gt_${mapping.dataset}_scene${mapping.sceneNum}.viser&${cameraParams}`;
      } else {
        viserPath = `viser-client/?playbackPath=../viser-scenes/exported/${currentMethod}_${mapping.dataset}_scene${mapping.sceneNum}.viser&${cameraParams}`;
      }
      
      document.getElementById('viser-frame').src = viserPath;
      
      // Update labels
      const methodName = document.querySelector(`.method-btn[data-method="${currentMethod}"]`).dataset.name;
      document.getElementById('current-method-name').textContent = methodName;
      document.getElementById('current-scene-name').textContent = `Scene ${currentScene}`;
    }

    // Scene thumbnail click handlers
    document.querySelectorAll('.scene-thumb').forEach(thumb => {
      thumb.addEventListener('click', function() {
        // Remove selection from all thumbnails
        document.querySelectorAll('.scene-thumb').forEach(t => {
          t.style.border = '3px solid transparent';
          const label = t.querySelector('p');
          label.style.background = '#ddd';
          label.style.color = '#333';
          label.innerHTML = label.textContent.replace('<strong>', '').replace('</strong>', '');
        });
        
        // Add selection to clicked thumbnail
        this.style.border = '3px solid #3273dc';
        const label = this.querySelector('p');
        label.style.background = '#3273dc';
        label.style.color = 'white';
        label.innerHTML = `<strong>${label.textContent}</strong>`;
        
        currentScene = this.dataset.scene;
        updateVisualization();
      });
    });

    // Method button click handlers
    document.querySelectorAll('.method-btn').forEach(btn => {
      btn.addEventListener('click', function() {
        // Remove selection from all buttons
        document.querySelectorAll('.method-btn').forEach(b => {
          const color = methodColors[b.dataset.method];
          b.style.background = 'white';
          b.style.color = '#333';
          b.style.border = `2px solid ${color}`;
        });
        
        // Add selection to clicked button
        const color = methodColors[this.dataset.method];
        this.style.background = color;
        this.style.color = 'white';
        this.style.border = `2px solid ${color}`;
        
        currentMethod = this.dataset.method;
        updateVisualization();
      });
    });
  })();
</script>



<!-- Ablations -->
<section class="section has-background-light" style="padding-top: 0rem; margin-top: 0rem;">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">Where Is 3D Encoded in Diffusion Models?</h2>

      <img src="./static/assets/abl.png" alt="Layer‚Äìtimestep ablations" style="width: 100%; border-radius: 12px;">

      <div style="display: flex; justify-content: space-around; width: 100%; text-align: center;">
        <span style="flex: 1;">(a) WAN2.1</span>
        <span style="flex: 1;">(b) Open-Sora2.0</span>
        <span style="flex: 1;">(c) CogVideoX</span>
      </div>
      <br>

      <div class="content has-text-left">
        <p>
          For video generators, we study which diffusion layer and timestep yield the most 3D-aware features by sweeping over three network layers and four denoising timesteps. Across all the models we study, the optimum is consistent: mid-network layers combined with an early-but-not-first time step, are significantly better than other layers and time steps. 
        </p>
      </div>
    </div>
  </div>
</section>


<!-- VidFM features for VGGT -->
<section class="section" style="padding-top: 0rem; margin-top: 0rem;">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">VidFM Features Improve Feedforward 3D (VGGT)</h2>

    <div class="content has-text-left">
      <p>
        VidFM features aren't just ‚Äúdiagnostically 3D-aware‚Äù, they are also practically useful, at least when 3D data and compute are limited. We replace the standard DINO-based feature backbone in VGGT with frozen WAN2.1-14B features, and train both models under a matched compute budget on CO3Dv2 and DL3DV. The VidFM-feature variant improves point/depth/pose metrics substantially, highlighting the advantage of video foundation model features for feedforward 3D reconstruction under limited 3D data.
      </p>
    </div>

    <div class="table-container">
      <table class="table is-striped is-hoverable is-fullwidth">
        <thead>
          <tr>
            <th rowspan="2">Method</th>
            <th colspan="4" class="has-text-centered">CO3Dv2</th>
            <th colspan="4" class="has-text-centered">DL3DV</th>
          </tr>
          <tr>
            <th>Point Err (‚Üì)</th>
            <th>Depth Err (‚Üì)</th>
            <th>AUC@5 (‚Üë)</th>
            <th>AUC@30 (‚Üë)</th>
            <th>Point Err (‚Üì)</th>
            <th>Depth Err (‚Üì)</th>
            <th>AUC@5 (‚Üë)</th>
            <th>AUC@30 (‚Üë)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Original VGGT (DINO)</td>
            <td>0.476</td><td>0.205</td><td>0.076</td><td>0.565</td>
            <td>2.751</td><td>0.518</td><td>0.058</td><td>0.363</td>
          </tr>
          <tr>
            <td><strong>VidFM-VGGT (WAN2.1-14B features)</strong></td>
            <td><strong>0.289</strong></td><td><strong>0.145</strong></td><td><strong>0.178</strong></td><td><strong>0.718</strong></td>
            <td><strong>1.034</strong></td><td><strong>0.319</strong></td><td><strong>0.183</strong></td><td><strong>0.686</strong></td>
          </tr>
        </tbody>
      </table>
    </div>

    </div>
  </div>
</section>


<!-- BibTeX -->
<section class="section has-background-light" id="BibTeX">
  <div class="container is-max-desktop content">
    <div class="hero-body">
      <h2 class="title has-text-centered">BibTeX</h2>
    <pre><code>@article{huang2025vidfm3d,
  title   = {How Much 3D Do Video Foundation Models Encode?},
  author  = {Huang, Zixuan and Li, Xiang and Lv, Zhaoyang and Rehg, James M.},
  booktitle = {arXiv preprint arXiv:2512.19949},
  year    = {2025}
}</code></pre>
    </div>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      This website template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
    </div>
  </div>
</footer>

</body>
</html>
